---
title: "Predicting Loan Defaults"
author: "Karolina Frieszova"
date: "03/03/2021"
output:
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr, warn.conflicts = FALSE)
# Suppress summarise info
options(dplyr.summarise.inform = FALSE)

iris %>% 
  group_by(Species) %>% 
  summarise(
    Sepal.Length.mean = mean(Sepal.Length)
  )
```


# INTRODUCTION

### Aim of the project

The project aimed to develop a predictive machine learning model on data provided by an online loan provider called LendingClub to aid their understanding and risk assessment of who they should lend to and who is likely to default in the future. 

### Business Understanding

LendingClub is a US peer-to-peer lending company and the world's largest peer-to-peer lending platform. LendingClub enables borrowers to create unsecured personal loans. Investors can search and browse the loan listings on the LendingClub website and select loans that they want to invest in based on the information supplied about the borrower. It is an online financial application service that directly connects individual investors and loan borrowers to establish credit relationships and complete the transaction procedures through an online platform, without the intermediaries of commercial banks. Investors make money from the interest on these loans. LendingClub makes money by charging borrowers an origination fee and investors a service fee. 

### Business Problem

So what is the business problem that we are trying to solve? The online lending platform has brought opportunities to investors. But at the same time, they are faced with the risk of user loan default, which is related to the sustainable and healthy development of LendingClub’s platform. Investors want to avoid funding loans to someone who won’t pay them back. Equally, they do not want to miss the opportunity to lend to someone who would pay back.

### Project Goal

So my project was basically about finding the right characteristics of an applicant and establishing an optimal threshold of accepted applications to maximise the profit of the business.

### Data Source

Data-set used contains historical data from LendingClub. The data is openly available on their website.

### References

Original Data Source [LendingClub website](https://www.lendingclub.com/investing/peer-to-peer)

General information about business [Wikipedia page](https://en.wikipedia.org/wiki/LendingClub)

Different loan statuses explained [LendingClub HelpPage](https://help.lendingclub.com/hc/en-us/articles/215488038-What-do-the-different-Note-statuses-mean-?fbclid=IwAR0DVRxmLkmOGPSwOz0FpWiJJBRNE2KuW0lvnbp5Mq7d9cZHy6g7dOcKMOk)

### Method

To build the predictive model I used supervised machine learning method (making predictions from labeled data), specifically, a logistic regression algorithm as a classifier for loan status. 

Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable with two possible values, such as loan paid or loan charged-off.

But before I could start building my binary classifier, I had to curry out vast data cleaning.

# DATA PREPERATION

### Data Introdustion

About the dataset, I was working with. It contains information about all loans, both past, and current, provided by LendingCub in 5 years, from 2007 to 2011. Each row represents one borrower and his or her information. Just above 42500 loans with information such as loan amount, installment, annual income, title, postcode, and much more.

### Data Cleaning

#### Packages used
```{r message=FALSE, echo=TRUE,cache=FALSE, results=TRUE, warning=FALSE, comment=FALSE, warning=FALSE}
library(tidyverse)
library(here)
library(janitor)
library(broom)
library(pROC)
library(modelr)
library(glmulti)
library(caTools)
library(ROCR)
library(ggplot2)
library(ggthemes)
library(cvms)
library(ROCit)
library(caret)
```

#### Reading in data and joing tables

The data-set was joined with two data-sets that added a state name to the zip code and reduced the risk grading system from 35 grades to 7.

```{r message=FALSE, echo=TRUE,cache=FALSE, results=TRUE, warning=FALSE, comment=FALSE, warning=FALSE}
 # read in data
grade_info <- read_csv(here("raw_data/grade_info.csv"))
lcd_dictionary <- read_csv(here("raw_data/LCDataDictionary.csv")) %>% 
  clean_names()
lending_club_loans <- read_csv(here("raw_data/lending_club_loans.csv"))
state_names <- read_csv(here("raw_data/state_names_info.csv"))

# add seven loan classifiers which indicate different levels of risk and corresponding returns
lending_club_loans <- left_join(lending_club_loans, grade_info, by = "sub_grade")

# add state names
lending_club_loans <- left_join(lending_club_loans, state_names, 
                                by = c("addr_state" = "state_abb"))
```

#### Initial variable reduction

The dataset contains over 100 different columns that LendingClub collects during different stages of the loan process. The project aimed to build a model that works with data that the company holds on applicants before the decision has been made about whether to grant the applicant loan. Therefore, I dropped variables containing information that are not present during the initial stage. 

- drop the variables with over 50% of missing values
- drop variables with one constant value

```{r}
# remove columns in which stored value is only NA
lending_loans <- lending_club_loans %>% 
  select(-c(total_il_high_credit_limit, mths_since_last_major_derog, num_bc_tl,
            mths_since_rcnt_il, mths_since_recent_bc, mths_since_recent_bc_dlq, 
            mths_since_recent_inq, mths_since_recent_revol_delinq, total_bc_limit,
            total_bal_ex_mort, tot_hi_cred_lim, percent_bc_gt_75, pct_tl_nvr_dlq, 
            num_tl_120dpd_2m, num_tl_30dpd, num_tl_90g_dpd_24m, num_tl_op_past_12m,
            num_sats, num_rev_tl_bal_gt_0, num_rev_accts, num_op_rev_tl, num_il_tl,
            num_bc_sats, num_actv_rev_tl, num_actv_bc_tl, num_accts_ever_120_pd,
            mort_acc, mo_sin_rcnt_tl, mo_sin_old_il_acct, mo_sin_old_rev_tl_op,
            mo_sin_rcnt_rev_tl_op, bc_util, bc_open_to_buy, avg_cur_bal, 
            acc_open_past_24mths, inq_last_12m, total_cu_tl, total_rev_hi_lim,
            inq_fi, all_util, max_bal_bc, open_rv_24m, open_rv_12m, il_util,
            total_bal_il, open_il_24m, open_il_12m, open_il_6m, open_acc_6m,
            tot_cur_bal, tot_coll_amt, verification_status_joint, dti_joint, 
            annual_inc_joint))

# remove data-sets
rm(grade_info, state_names, lending_club_loans)

# further column reduction
lending_loans <- lending_loans %>% 
  select(-c(id, member_id, url, desc, last_pymnt_d, last_pymnt_amnt, last_credit_pull_d, # unrequited
            sub_grade, addr_state, # replaced with abbreviation
            tax_liens, # only one different value
            policy_code, # only policy code = 1 or NA, no policy code = 2
            initial_list_status, # False, NA
            collections_12_mths_ex_med, chargeoff_within_12_mths, # 0, NA 
            title, # high correlation with purpose
            emp_title, next_pymnt_d, # high cardinalty and a lot missing 
            zip_code, # earliest_cr_line, # high cardinalty, not required
            funded_amnt, funded_amnt_inv, # highly correlated with loan_amount
            pymnt_plan, # only one True value - rest False
            total_pymnt_inv, # highly correlated with total_pymnt
            acc_now_delinq, # 4 rows = 1 all have loan_status = fully paid
            delinq_amnt, # 2 rows with loan_status = fully paid
            application_type, # all applications are individual
            mths_since_last_delinq, # 63.3 % missing values
            mths_since_last_record, # 91.4% missing vales
            out_prncp_inv, # highly correlated with out_prncp
            last_fico_range_low, # highly correlated with last_fico_range_high
            total_rec_prncp # highly correlated with total_pymnt (principal is the amount you borrowed without interest)
            )) %>% 
  drop_na(open_acc) # remove 32 rows as this rows are missing across multiple columns
```

### 2.3 Feature engineering

The target variable in my model is loan_status. The feature has 9 different values. Here they are explained:

- *Current:* Loan is up to date on all outstanding payments. 
- *In Grace Period:* Loan is past due but within the 15-day grace period. 
- *Fully paid:* Loan has been fully repaid, either at the expiration of the 3- or 5-year year term or as a result of a prepayment.
- *Default:* Loan has not been current for an extended period of time.
- *Charged Off:* Loan for which there is no longer a reasonable expectation of further payments. Charge off typically occurs when a loan is 120 days or more past due.  
- *Late (16-30):* Loan has not been current for 16 to 30 days. 
- *Late (31-120):* Loan has not been current for 31 to 120 days.

I set loan status as my target variable and reduced the number of observations by selecting only past loans as these have known outcomes. I encoded it to represent only two possibilities: paid and default loans. 

As a result, the dataset was reduced by approximately 500 rows to still almost 42000 in total.

Tackling features with high cardinality: Our data-set has categorical variables containing many multiple labels. Then by using one-hot encoding, we will expand the feature space dramatically. For this reason, the cardinality of some variables was reduced. Closer analysis on these variables was performed initially.

The features related to the past incidences of delinquency, number of inquiries in the past 6 months, number of derogatory public records, late fees received to date, and post charge off gross recovery were converted to the logical variable. These values were assigned TRUE and where there wasn't record FALSE was assigned.

The missing values in the Revolving line utilization rate were replaced with median imputation. 

Next, I created a feature that represents the monthly employment expense of the user's repayment expense as a percentage of the monthly salary. 

Variable 'out_prncp' was removed as it doesn't store any values for Fully Paid or Charged Off.

Lastly, after closer analysis, I identified that we can't draw any strong conclusions of whether the 'Charge Off' is affected by the state variable, so we won't be adding 'addr_state' to our model, and the same applies to the issue date.

I concentrated attention at the variables that LendingClub and its investors have about borrower during the initial stage before the decision on whether to grant a loan has beenn made.

```{r}
lending_loans <- lending_loans %>% 
  filter(!loan_status %in% c("Current", "In Grace Period", "Late (31-120 days)", "Late (16-30 days)", "Default")) %>%
  mutate(loan_status = case_when(loan_status == "Fully Paid" ~ T,
                                 loan_status == "Does not meet the credit policy. Status:Fully Paid" ~ T,
                                 loan_status == "Charged Off" ~ F,
                                 loan_status == "Does not meet the credit policy. Status:Charged Off" ~ F
  )) %>% # abstraction: loan status paid = T, charged off = F
  rename("addr_state" = "state_name") %>% 
  mutate(delinq_2yrs = ifelse(delinq_2yrs == 0, F, T), # past-due incidences of delinquency in the borrower's credit file for the past two years
         inq_last_6mths  = ifelse(inq_last_6mths == 0, F, T),
         pub_rec = ifelse(pub_rec == 0, F, T), # derogatory public records
         total_rec_late_fee = ifelse(total_rec_late_fee == 0, F, T), # Late fees received to date
         recoveries = ifelse(recoveries == 0, F, T), # post charge off gross recovery
         collection_recovery_fee = ifelse(collection_recovery_fee == 0, F, T), # post charge off collection fee
         int_rate_pct = str_remove_all(int_rate, "[%]"),
         home_ownership = recode(home_ownership, "NONE" = "OTHER"),
         int_rate_pct = as.numeric(int_rate_pct), # convert interest rate to numeric
         issue_d = str_remove_all(issue_d, "[0-9-]"), # remove year, leave month
         emp_length = case_when(emp_length == "10+ years" ~ "10+ years",
                                emp_length %in% c("9 years", "8 years", "7 years", "6 years") ~ "above 5 years",
                                emp_length %in% c("5 years", "4 years", "3 years", "2 years") ~ "2 - 5 years",
                                emp_length %in% c("1 year", "< 1 year") ~ "1 and under",
                                emp_length == "n/a" ~ "unknown"),
         addr_state = coalesce(addr_state, "unknown"),
         revol_util_pct = str_remove_all(revol_util, "[%]"), # the amount of credit the borrower is using relative to all available revolving credit
         revol_util_pct = as.numeric(revol_util_pct), # convert to numeric
         revol_util_pct = coalesce(revol_util_pct, median(revol_util_pct, na.rm = TRUE)), # replace missing values with median
         pub_rec_bankruptcies = as.character(pub_rec_bankruptcies),
         pub_rec_bankruptcies = case_when(pub_rec_bankruptcies == "0" ~ "no", 
                                          pub_rec_bankruptcies == "1" ~ "yes",
                                          pub_rec_bankruptcies == "2" ~ "yes", 
                                          TRUE ~ "unknown"), # public record bankruptcies
         install_mth_pct = round((installment * 100) / (annual_inc/12), 2), # monthly installment expense %
         fico_range_avg = (fico_range_low + fico_range_high) / 2,
         earliest_cr_line = str_remove_all(earliest_cr_line, "[A-Za-z-]"),
         earliest_cr_line = case_when(earliest_cr_line >= 2000 ~ "00s",
                                      earliest_cr_line >= 1940 ~ "40s-90s",
                                      T ~ "unknown"),
         term_36 = ifelse(term == "36 months", T, F),
         purpose = case_when(purpose %in% c("home_improvement", "house") ~ "house related",
                             purpose %in% c("vacation", "wedding", "car", "major_purchase") ~ "personal",
                             T ~ as.character(purpose)), #  where relevant reduce the amount of variable labels
         purpose = as.factor(purpose)
         ) %>% # if it is not 36, we know it will be 60 months
  mutate_if(is_character, as_factor) %>% 
  select(-c(collection_recovery_fee, int_rate, revol_util, fico_range_low, fico_range_high, addr_state, issue_d, out_prncp, last_fico_range_high, term, recoveries, total_rec_late_fee))
```

#### Check for aliases in the independent variables
```{r, results = FALSE}
alias(loan_status ~ ., data = lending_loans)
```
All the variables are independent.

#### Correlation between the features
Now let's check correlation between the features to decide which variables to include in the model. As we can see there is a strong positive correlation between loan amount and three other variables (installment (0.93), total_pymnt (0.88), and total_rec_int (0.73)), therefore, we can drop them.

```{r, results = FALSE}
# check correlation between numeric features
cor(lending_loans[, sapply(lending_loans, class) == "numeric"])

# remove highly correlated variables
lending_loans <- lending_loans %>% 
  select(-c(installment, total_pymnt, total_rec_int))
```

# EXPLORATORY ANALYSIS

A principal source of biases the ML models is from the data used to train it. In order to understand our data we will perform Exploratory Data Analysis. The main purpose of exploratory analysis is to help look at data before making any assumptions. It is one of the most important steps before building a model in order to identify obvious errors, as well as better understand patterns within the data, detect outliers or find interesting relations among the variables.


```{r}
loan_status_graph <- lending_loans %>% 
  ggplot(aes (x = loan_status))+
  geom_bar(fill = c("#e41a1c", "#4daf4a"))+
  labs(title = "Distribution of loans by repayment status",
       y = "Count",
       x = "Loan status (Charged off = False, Paid = True)")+
  theme_economist()
```


### Loan amount

```{r}
lending_loans %>% 
  ggplot(aes(x = loan_amnt))+
  geom_histogram(fill = "#377eb8", bins = 30)+
  labs(title = "Distribution of Loan Amount",
       x = "Loan Amount",
       y = "Count")+
  theme_economist()
```

```{r}
lending_loans %>% 
  ggplot(aes(x = loan_status, y = loan_amnt))+
  geom_boxplot(fill = c("#e41a1c", "#4daf4a"))+
  coord_flip()+
  labs(title = "Loan Amount by Loan Status",
       x = "Loan Status",
       y = "Loan Amount")+
  theme_economist()
  
```

Charged-off loans tend to have higher loan amounts. 


### Interest Rate

```{r}
lending_loans %>% 
  ggplot(aes(x = int_rate_pct))+
  geom_histogram(fill = "#377eb8", bins = 30)+
  labs(title = "Distribution of Interest Rate %",
       x = "Interest Rate %",
       y = "Count")+
  theme_economist()
```

```{r}
lending_loans %>% 
  ggplot(aes(x = loan_status, y = int_rate_pct))+
  geom_boxplot(fill = c("#e41a1c", "#4daf4a"))+
  coord_flip()+
  labs(title = "Interest Rate % by Loan Status",
       x = "Loan Status",
       y = "Interest Rate %")+
  theme_economist()
```

Charged-off loans tend to have much higher interest rates.


### Home Ownership

```{r}
lending_loans %>% 
  ggplot(aes(x = home_ownership, fill = loan_status))+
  geom_bar()+
  labs(title = "Loan Status by Home Ownership",
       x = "Home Ownership",
       y = "Count",
       fill = "Loan Status")+
  scale_fill_manual(values = c("#e41a1c", "#4daf4a"))+
  theme_economist()
```

Most of the borrowers are renting or have mortgage. Highest amount of charged-off loans is in the 'rent' category.


### Verification Status

```{r}
lending_loans %>% 
  ggplot(aes(x = verification_status, fill = loan_status))+
  geom_bar()+
  labs(title = "Loan Status by Verification Status",
       x = "Verification Status",
       y = "Count",
       fill = "Loan Status")+
  scale_fill_manual(values = c("#e41a1c", "#4daf4a"))+
  theme_economist()
```

Large portion of the borrowers is not verified.


### Fico Range

```{r}
lending_loans %>% 
  ggplot(aes(x = fico_range_avg))+
  geom_histogram(fill = "#377eb8", bins = 30)+
  labs(title = "Distribution of Average Fico Range",
       x = "Mean Fico Range",
       y = "Count")+
  theme_economist()
```


```{r}
lending_loans %>% 
  ggplot(aes(x = loan_status, y = fico_range_avg))+
  geom_boxplot(fill = c("#e41a1c", "#4daf4a"))+
  coord_flip()+
  labs(title = "Avg Fico Range by Loan Status",
       x = "Loan Status",
       y = "Meam Fico Range")+
  theme_economist()
```

Charged-off loans tend to have much smaller Fico Range. (*US Fico Range is similar UK's Credit Score)


### Purpose

I looked at the distribution of loans by purpose. I could see that taking loans for a small business and education had the highest percentage of default loans, however, there weren’t overall so many loans taken for this purposes.

```{r}
# investigate the amount of charged off loans by purpose category
lending_loans %>% 
  group_by(purpose) %>% 
  summarise(count = n()) %>% 
  ggplot(aes(x = reorder(purpose, count), y = count))+
  geom_col(fill = "#377eb8")+
  coord_flip()+
  labs(title = "Loans by Purpose",
       y = "Frequency",
       x = "Purpose")+
  theme_economist()
```

```{r}
# percentage loans by purpose category and Loan status
lending_loans %>% 
  group_by(purpose, loan_status) %>% 
  summarise(purchase_count = n()) %>% 
  mutate(pct = round(purchase_count * 100 / sum(purchase_count))) %>%
  ggplot(aes(x = purpose, y = pct, fill = loan_status))+
  geom_col()+
  coord_flip()+
  labs(title = "% of Loans by Purpose and Loan Status",
       y = "Frequency",
       x = "Purpose",
       fill = "Fully Paid")+
  scale_fill_manual(values = c("#e41a1c", "#4daf4a"))+
  theme_economist()
```


### Annual Income

Summary statistics of annual income. We can see that there are few outliers. However, we won't be dealing with them. After closer inspection we could see that our two borrowers with highest annual income took loan for home improvements and it was fully paid so we can well assume that this was the case.

```{r}
lending_loans %>% 
  mutate(annual_inc = annual_inc / 1000) %>% 
  group_by(annual_inc) %>% 
  summarise(frequency = n()) %>% 
  ggplot(aes(x = annual_inc, y = frequency))+
  geom_point(color = "#377eb8")+
  labs(title = "Annual income",
       y = "Frequency",
       x = "Annual income (unit 1000)")+
  theme_economist()
```


```{r, results = FALSE}
lending_loans %>% 
  select(annual_inc, purpose, loan_status) %>% 
  filter(annual_inc > 3000000)
```


### Grade

Distribution of loans by grading system is another information that can be beneficial for LendingClub. Loan grading is a classification system that involves assigning a quality score to a loan based on a borrower's credit history, quality of the collateral, and the likelihood of repayment of the principal and interest. I have calculated the percentage of each grading for two loan statuses. We can see that the proportion of loans that are not fully paid increases for poorer risk grades.

```{r}
lending_loans %>% 
  mutate(grade = fct_relevel(grade, "A", "B", "C", "D", "E", "F", "G")) %>% 
  ggplot(aes(x = grade))+
  geom_bar(fill = "#377eb8")+
  labs(title = "Loans by Grade Category",
       y = "Count",
       x = "Grade")+
  theme_economist()
```

```{r}
lending_loans %>% 
  select(grade, loan_status) %>% 
  group_by(grade, loan_status) %>% 
  mutate(grade = fct_relevel(grade, "A", "B", "C", "D", "E", "F", "G")) %>% 
  summarise(grade_count = n()) %>% 
  mutate(pct = round(grade_count * 100 / sum(grade_count))) %>% 
  ggplot(aes(x = grade, y = pct, fill = loan_status))+
  geom_col()+
  labs(title = "% of Loans by Grading System and Loan Status",
       fill = "Fully Paid",
       y = "Percentage",
       x = "Grade")+
  scale_fill_manual(values = c("#e41a1c", "#4daf4a"))+
  theme_economist()
```


# MODEL BUILDING

### Feature selection

Next step, I got to building my logistic regression model. After I assured that my features fulfill the model assumptions on the independence of two variables, dependent variable to be binary, and no high correlation between independent variables. 

We can conclude which features we be included in the model. The model will include following 21 independent variables and one outcome variable.

### Data Spliting

Before building the model we need to split our data into training and test set. We are going to train the classifier on the training data and then we are going to test how good our classifier is on the test data. I have decided to use single test set method where I split data-set into into a training set and testing set using conventional 80 to 20 random split. It assured that my outcome variable is well-balanced in each piece so our test set is representative of our training set.

```{r, results = FALSE}
# Splitting data set into training and test set
set.seed(42) # making results reproducible

# test/train splitting
split = sample.split(lending_loans$loan_status, SplitRatio = 0.8)
train = subset(lending_loans, split == TRUE)
test = subset(lending_loans, split == FALSE)
```

Sample balance
At this stage it is important to address that our outcome variable loan status is significantly unbalanced between the paid and default classes, approximately 5 to 1 ratio. This is understandably due to the nature of the business. 
```{r}
table(lending_loans$loan_status)
```

#### Distribution of training and testing sets

Check the distribution of outcome in the training and testing sets to see that they are roughly comparable.

```{r}
# check the distribution of outcome in the training and testing
train %>%
  tabyl(loan_status)

test %>%
  tabyl(loan_status)
```

## Logistic regression
### First Model

#### First Model Fit

After we have carried out data cleaning and data analysis we fit our predictors into the generalised linear model function for logistic regression.

```{r, results = FALSE}
# logistic regression with all predictors
log_reg_model_1 <- glm(loan_status ~ ., data = train, family = binomial(link = "logit"))
```


#### Plotting ROC curve

The receiving operating characteristic (ROC) is a visual measure of classifier performance. Using the proportion of positive data points that are correctly considered as positive and the proportion of negative data points that are mistakenly considered as positive, we generate a graphic that shows the trade off between the rate at which you can correctly predict something with the rate of incorrectly predicting something. Ultimately, we’re concerned about the area under the ROC curve, or AUC. That metric ranges from 0.50 to 1.00, and values above 0.80 indicate that the model does a good job in discriminating between the two categories which comprise our target variable. 

```{r, message=FALSE}
loans_pred <- test %>%
  add_predictions(log_reg_model_1, type = "response")

roc_pred <- loans_pred %>%
  roc(response = loan_status, predictor = pred)

roc_curve <- ggroc(data = roc_pred, legacy.axes = TRUE) +
  coord_fixed()+
  theme_economist()

roc_curve
```


#### Area under the ROC Curve

If we calculate the area under the curve (AUC), we can use this as a measure of performance. Better classifiers have curves closer to the top left point, and so have larger AUC values. AUC will range from 0.50 - 1.00. 

```{r, message=FALSE, results=FALSE}
roc_train <- train %>%
  add_predictions(log_reg_model_1, type = "response") %>%
  roc(response = loan_status, predictor = pred)

auc(roc_train)

roc_test <- test %>%
  add_predictions(log_reg_model_1, type = "response") %>%
  roc(response = loan_status, predictor = pred)

auc(roc_test)
```

The classification accuracy is relatively low 0.70 for training set and 0.69 for test set.


#### Confusion Matrix

```{r, results = FALSE}
# confusion matrix

list(
  log_reg_model_1= table(test$loan_status,  loans_pred$pred > 0.8) %>% prop.table() %>% round(3)
)
```
true positives (Bottom-right quadrant): these are cases in which we predicted the customer would Fully pay and they did. - 67% accuracy.

true negatives (Top-left quadrant): We predicted loan will be Charged off, and the customer did not fully repay the loan. - 7% accuracy.

false positives (Top-right quadrant): We predicted Charged Off, but they actually Fully Paid. (Also known as a “Type I error.”) - 8%

false negatives (Bottom-left): We predicted loan Fully paid, but it was actually Charged off. (Also known as a “Type II error.”) - 18%

The results show that 67% of the predicted observations are true positive and only 7% are true negative. The model has larger type II error, 18% of predicted observations are false positive in which the model predicts borrower not to fully pay the loan but they actually paid off. And the model has type I error - false negative - 0.8% in which it predicts borrower to fully pay the loan but they didn't.

Our model is fairly poor in predicting who won't pay off the loan.


### Second Model


#### Automated Model Selection

At this stage, I used automated model selection to look for the best model and to further decrease the feature selection. It returned a model with 11 predictors plus the outcome variable. 
```{r eval=FALSE}
glmulti_search_all_loans <- glmulti(
  loan_status ~ ., 
  data = train,
  level = 1,               # Interactions considered
  method = "g",            # "g" the genetic algorithm (recommended for large sets)
  crit = "bic",            # BIC as criteria
  confsetsize = 10,        # Keep 10 best models
  plotty = F, 
  report = T,              # No plots, but provide interim reports
  fitfunction = "glm",     # glm function
  family = binomial(link = "logit")) # binomial family for logistic regression

summary(glmulti_search_all_loans)
```


#### Best Model Fit

Which I had then fitted into the generalised linear model function for logistic regression.
```{r}
# second logistic regression model from glmulti
log_reg_model <- glm(loan_status ~ emp_length + purpose + pub_rec_bankruptcies +  annual_inc + inq_last_6mths + revol_bal + int_rate_pct + revol_util_pct + install_mth_pct + fico_range_avg + term_36, data = train, family = binomial(link = "logit"))
```

#### Model's performance

As we know logistic regression has binary values for dependent variable and the model outputs estimated probabilities. We can see how the model uses these probability values. We have to set a threshold above which we say the loan application is predicted to be paid, and below which it’s predicted to be default.

```{r}
# add column with estimated probabilities
test_data_with_predictors <- test %>%
  add_predictions(log_reg_model, type = "response")

# example of outcomes when using different threshold values
test_data_with_predictors <- test_data_with_predictors %>%
  mutate(pred_thresh_0.5 = pred >= 0.5) %>% 
  mutate(pred_thresh_0.8 = pred >= 0.8)

test_data_with_predictors %>% 
  select(loan_status, pred, pred_thresh_0.5, pred_thresh_0.8) %>% 
  head(10)
```


#### Plotting ROC curve
Selecting an optimal threshold value is often challenging. A receiver characteristic curve or ROC curve, can help us decide which value of the threshold is the best. The ROC curve is shown on the down left side of the page. The sensitivity, or true positive rate of the model is shown on the y axis, and the false positive rate or 1 minus the specificity is given on the x axis. The line shows how these two outcome measures vary with different threshold values. The ROC curve starts at point(0,0) which corresponds to threshold value 1. The ROC curve always ends at point (1,1), which corresponds to the threshold value of 0. Around the point(0.35, 0.65), we are correctly labeling about 65% of paid loans but have a false positive rate of 35%. 
So which threshold value should we pick? We should select the best threshold for the tradeoff we want to make. 

```{r}
predict_train = predict(log_reg_model, type = "response")

rocr_pred = ROCR::prediction(predict_train, train$loan_status)
rocr_perf = ROCR::performance(rocr_pred, "tpr", "fpr")
plot(rocr_perf, colorize = T, print.cutoffs.at = seq(0.55,1,0.05), text.adj = c(-0.2, 1.7), ylab = "TPR (sensitivity)", xlab = "\n FPR (1 - specificity/TNR)")
```
#### Finding optimal cutoff - Youdens index method
By default the threshold value is set to 0.5 which works well for balanced datasets. But in my case I worked with an unbalanced dataset where most of the clients felt into class one paid off loan. We could actually try to address this issue by finding an optimal threshold. On the right hand side you could see Youden’s Index graph which gives equal weight to false positive and false negative values, it means the same proportion of miscalculated results. Using this method, I calculated the threshold value as 0.85 which is pretty close to the disproportion of the target variable 5 to 1.
```{r}
## make the score and class
class <- log_reg_model$y
# score = log odds
score <- qlogis(log_reg_model$fitted.values)
## rocit object
rocit_emp <- rocit(score = score,
class = class,
method = "emp")
rocit_bin <- rocit(score = score,
class = class,
method = "bin")
rocit_non <- rocit(score = score,
class = class,
method = "non")
summary(rocit_emp)
summary(rocit_bin)
summary(rocit_non)
## Plot ROC curve
plot(rocit_emp, col = c(1,"gray50"),
legend = FALSE, YIndex = TRUE)
```

#### Sensitivity and Specificity

```{r}
loans_pred <- test %>%
  add_predictions(log_reg_model, type = "response")
list(
  log_reg_model= table(test$loan_status,  loans_pred$pred > 0.85) %>% prop.table() %>% round(3)
)
```
```{r}
test_data_with_predictors <- test_data_with_predictors %>%
  mutate(pred_thresh_0.85 = pred >= 0.85)

test_data_with_predictors %>%
  tabyl(loan_status, pred_thresh_0.85)

sensitivity = 4550 / (4550 + 2560)
sensitivity

specificity = 818 / (818 + 464)
specificity
```

```{r}
accuracy = (818 + 4550) / (818 + 464 + 2560 + 4550)
accuracy
```

```{r}
error = 1 - accuracy
error
```

### Evaluation of the final/second model

#### Confusion Matrix and Statistics

```{r}
thres_0.85 <- test_data_with_predictors %>%
  mutate(pred_thresh_0.85 = pred >= 0.85)
predicted  <- thres_0.85$pred_thresh_0.85 %>% 
  as.factor()
expected <- thres_0.85$loan_status %>% 
  as.factor()
df <- data.frame(predicted, expected)
results <- confusionMatrix(data = predicted, reference = expected)
results
```
I have adjusted my classification model by applying the optimal threshold 0.85 as a cut off point where to label loan applications as paid or default type. 
On the left side, is my final confusion matrix converted to percentage and visualised as a heat map. As you can see the logistic regression model is better at predicting who will pay the loan but or true positive at 54%. On the other hand, the model is very poor at predicting the defaults with true negative coming to under 10%.
The model has large type II error, false negative when the model predicts loan to be default but it is actually paid off.

```{r}
d_binomial <- tibble("prediction" = thres_0.85$pred_thresh_0.85,
            "actual" = thres_0.85$loan_status)
basic_table <- table(d_binomial)
cfm <- tidy(basic_table)

plot_confusion_matrix(cfm, 
                      target_col = "actual", 
                      prediction_col = "prediction",
                      counts_col = "n",
                      add_row_percentages = FALSE,
                      add_col_percentages = FALSE)
```


#### Area under the ROC Curve

```{r, message=FALSE, results=FALSE}
roc_train_2 <- train %>%
  add_predictions(log_reg_model, type = "response") %>%
  roc(response = loan_status, predictor = pred)

auc(roc_train_2)

roc_test_2 <- test %>%
  add_predictions(log_reg_model, type = "response") %>%
  roc(response = loan_status, predictor = pred)

auc(roc_test_2)
```

The classification accuracy is relatively low 0.7054 for training set and 0.6977 for test set.

Next we could calculate the area under the curve or AUC. We could use this as another measure of performance. The matrics ranges from 0.5 to 1, and values above 0.8 generally indicate that the model does a good job in describing between two categories which comprise our target variable. Our model has an accuracy of 70% which is definitely better than the random guess but we could look at how to improve it.
```{r}
test_prob = predict(log_reg_model, newdata = test, type = "response")
test_roc = roc(test$loan_status ~ test_prob, plot = TRUE, print.auc = TRUE, percent = TRUE, col = "#377eb8", lwd = 4)
```

#### Feature Relative importance
Below is a bar graph showing the relative importance of the main features that have a direct impact on an applicant being likely to pay the loan back or fall into default. The highest impact has a purpose of loan for small business, followed by loan term of 36 months, and loan inquiry in the past 6 months.
```{r}
var_importance <- varImp(log_reg_model, scale = FALSE)

var_importance<- var_importance %>% 
  rownames_to_column(var = "predictor") %>% 
  clean_names()

var_importance <- var_importance %>% 
  arrange(desc(overall)) %>% 
  top_n(13)
```

```{r}
ggplot(var_importance)+
  aes(x = reorder(predictor, -overall), y = overall)+
  geom_col(fill = "#377eb8")+
  labs(title = "Feature Relative Importance",
       y = "Relative importance",
       x = "")+
  theme_economist()+
  theme(axis.text.x=element_text(angle=90, hjust=1))
  
```


# CONCLUSION
Our model performs better when predicting who is likely to pay off the loan in comparison to who won’t in the future. The model produces a large type II error of false-negative rate. We measured the model's performance also looking at the area under the curve which is equal to 70%, as I mentioned earlier we are generally looking for AUC to be above 80%. 
Possible reason why our model is not great at predicting who won’t pay back could be due to learning from the unbalanced dataset. The model learned from the training set with a significantly higher proportion of fully paid compared to default loans. Therefore, default loan is a rare event in our training data set.
Dealing with disproportionate classification is a common scenario in the investment and banking sector, where the classification accuracy could be a problematic measure and could lead us to make wrong decisions. 

Therefore, for this kind of model, the ROC curve is a better measure of model performance as it gives us a real picture of the quality of our model.  

For future model performance improvement, I would suggest applying additional methods such as strategy curve, cost-sensitive learning, under-sampling, over-sampling, re-training model with newer data. 
Also, other methods can be used for classification, for instance, random forest, neural networks, or naive Bayes. 

*Classification Accuracy is the number of correct predictions made divided by the total number of predictions made, multiplied by 100 to turn it into a percentage.